{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data extracted on 11 May 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Scrape(start_page,end_page,file_name): #End page is not inclusive\n",
    "    names,addresses,beds,baths,year_built,tenure,property_type,mrt,price,sqft = [],[],[],[],[],[],[],[],[],[]\n",
    "    housing = ['HDB','HDB 2 Rooms','HDB 3 Rooms','HDB 4 Rooms','HDB 5 Rooms','HDB Executive','Condo','Apartment','Walk-up','Landed','Executive Condo','Terraced House','Corner Terrace','Semi-Detached','Bungalow','Good Class Bungalow','Cluster House']\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    for num in range(start_page,end_page): #Iterate through the pages to obtain property listing links\n",
    "        driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "        driver.implicitly_wait(5)\n",
    "        driver.get('https://www.99.co/singapore/sale?page_num={}'.format(num))\n",
    "        time.sleep(3)\n",
    "        links =[]\n",
    "\n",
    "        soup=BeautifulSoup(driver.page_source, 'lxml')\n",
    "        for link in soup.find_all('a'): \n",
    "            if type(link.get('href'))==str and 'Search' in link.get('href') and 'page_num'not in link.get('href'):\n",
    "\n",
    "                links.append('https://www.99.co'+link.get('href'))\n",
    "\n",
    "\n",
    "        for c,i in enumerate(links):  #Iterate through the porperty links\n",
    "\n",
    "            driver.get(i)\n",
    "            time.sleep(1.5)\n",
    "            soup=BeautifulSoup(driver.page_source, 'lxml')\n",
    "            name = soup.find('h1', attrs={'class': '_3Wogd JMF8h lFqTi _1vzK2'})  #Getting property listing names\n",
    "            names.append(name.getText())\n",
    "\n",
    "\n",
    "            address = soup.find_all('p', attrs={'class': 'dniCg _3j72o _2rhE-'}) #Getting property address code\n",
    "            a = []\n",
    "            for i in address:\n",
    "                a.append(i.getText())\n",
    "            addresses.append(re.findall('(\\(D\\d\\d\\)|\\(D\\d\\))',''.join(a)))\n",
    "\n",
    "            bed = soup.find_all('p', attrs={'class': '_2sIc2 _29qfj _2rhE-'}) #Getting property number of bed rooms, bathrooms and sqft\n",
    "            temp1 = ['NA','NA','NA']\n",
    "            for i in bed:\n",
    "                if 'Bed'in i.getText().split() or 'Beds' in i.getText().split():\n",
    "                    temp1[0]=i.getText().split()[0]\n",
    "\n",
    "                elif 'Bath' in i.getText().split() or 'Baths' in i.getText().split():\n",
    "                    temp1[1]=i.getText().split()[0]\n",
    "\n",
    "                elif 'sqft' in i.getText().split():\n",
    "                    temp1[2]=i.getText().split()[0]\n",
    "                elif 'Studio' in i.getText().split():\n",
    "                    temp1[0]=i.getText()            \n",
    "                else:\n",
    "                    print('Please maunually get info for page{} and link with index {}'.format(num,c))\n",
    "\n",
    "            beds.append(temp1[0])\n",
    "            baths.append(temp1[1])\n",
    "            sqft.append(temp1[2])\n",
    "\n",
    "\n",
    "            info = soup.find_all('div', attrs={'class': '_3r4yN XCAFU'}) #Getting porperty tenure, year buildt and property type\n",
    "            temp = ['NA','NA','NA']\n",
    "\n",
    "            for i in info:\n",
    "                if 'year' in i.getText().lower():\n",
    "                    temp[0]=i.getText()[:2]\n",
    "                elif 'Freehold' in i.getText():\n",
    "                    temp[0]=i.getText()\n",
    "                elif re.findall('\\d\\d\\d\\d',i.getText())!=[]:\n",
    "                    temp[1]=i.getText()\n",
    "                elif i.getText() in housing:\n",
    "                    temp[2]=i.getText()   \n",
    "            tenure.append(temp[0])\n",
    "            year_built.append(temp[1])\n",
    "            property_type.append(temp[2])\n",
    "\n",
    "            mrts = soup.find_all('p', attrs={'class': '_2sIc2 _2rhE- _1c-pJ'}) #Getting property distance to MRT\n",
    "            temp2 = ['NA']\n",
    "            for i in mrts:\n",
    "                if re.findall('\\(\\d+ m\\)',''.join(i.getText()))!=[]:\n",
    "                        temp2[0]=re.findall('\\(\\d+ m\\)',''.join(i.getText()))\n",
    "            mrt.append(temp2[0])\n",
    "\n",
    "\n",
    "\n",
    "            prices = soup.find('h2', attrs={'class': '_1zGm8 _3na6W _1vzK2'}) #Getting property prices\n",
    "            price.append(prices.getText()[1:])\n",
    "\n",
    "        if len(names)==len(addresses) and len(addresses)==len(beds) and len(beds)==len(baths) and len(baths)==len(year_built) and len(year_built)==len(tenure) and len(tenure)==len(property_type) and len(property_type)==len(mrt) and len(mrt)==len(price):\n",
    "            print('All good for page number {}!'.format(num)) #Check that the data are all filled\n",
    "        else:\n",
    "            print('shit, houston we have a problem!')  #Break if error\n",
    "            break\n",
    "        driver.close()\n",
    "    data=pd.DataFrame(names,columns=['title'])\n",
    "    data['address_code']=addresses\n",
    "    data['beds']=beds\n",
    "    data['baths']=baths\n",
    "    data['year_built']=year_built\n",
    "    data['tenure']=tenure\n",
    "    data['property_type']=property_type\n",
    "    data['mrt_distance']=mrt\n",
    "    data['sqft']=sqft\n",
    "    data['price']=price\n",
    "    data.to_csv(file_name)   #Saving as a CSV file\n",
    "    print('Scraped Successfully! :)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good for page number 1!\n",
      "All good for page number 2!\n",
      "All good for page number 3!\n",
      "All good for page number 4!\n",
      "All good for page number 5!\n",
      "All good for page number 6!\n",
      "All good for page number 7!\n",
      "All good for page number 8!\n",
      "All good for page number 9!\n",
      "All good for page number 10!\n"
     ]
    }
   ],
   "source": [
    "Scrape(1,11,'scraped_pg1_pg10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good for page number 11!\n",
      "All good for page number 12!\n",
      "All good for page number 13!\n",
      "All good for page number 14!\n",
      "All good for page number 15!\n",
      "All good for page number 16!\n",
      "All good for page number 17!\n",
      "All good for page number 18!\n",
      "All good for page number 19!\n",
      "All good for page number 20!\n",
      "Scraped Successfully! :)\n"
     ]
    }
   ],
   "source": [
    "Scrape(11,21,'scraped_pg11_pg20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good for page number 21!\n",
      "All good for page number 22!\n",
      "All good for page number 23!\n",
      "All good for page number 24!\n",
      "All good for page number 25!\n",
      "All good for page number 26!\n",
      "All good for page number 27!\n",
      "All good for page number 28!\n",
      "All good for page number 29!\n",
      "All good for page number 30!\n",
      "Scraped Successfully! :)\n"
     ]
    }
   ],
   "source": [
    "Scrape(21,31,'scraped_pg21_pg30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good for page number 31!\n",
      "All good for page number 32!\n",
      "All good for page number 33!\n",
      "All good for page number 34!\n",
      "All good for page number 35!\n",
      "All good for page number 36!\n",
      "All good for page number 37!\n",
      "All good for page number 38!\n",
      "All good for page number 39!\n",
      "All good for page number 40!\n",
      "Scraped Successfully! :)\n"
     ]
    }
   ],
   "source": [
    "Scrape(31,41,'scraped_pg31_pg40.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good for page number 41!\n",
      "All good for page number 42!\n",
      "All good for page number 43!\n",
      "All good for page number 44!\n",
      "All good for page number 45!\n",
      "All good for page number 46!\n",
      "All good for page number 47!\n",
      "All good for page number 48!\n",
      "All good for page number 49!\n",
      "All good for page number 50!\n",
      "Scraped Successfully! :)\n"
     ]
    }
   ],
   "source": [
    "Scrape(41,51,'scraped_pg41_pg50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good for page number 51!\n",
      "All good for page number 52!\n",
      "All good for page number 53!\n",
      "All good for page number 54!\n",
      "All good for page number 55!\n",
      "All good for page number 56!\n",
      "All good for page number 57!\n",
      "All good for page number 58!\n",
      "All good for page number 59!\n",
      "All good for page number 60!\n",
      "Scraped Successfully! :)\n"
     ]
    }
   ],
   "source": [
    "Scrape(51,61,'scraped_pg51_pg60.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good for page number 61!\n",
      "All good for page number 62!\n",
      "All good for page number 63!\n",
      "All good for page number 64!\n",
      "All good for page number 65!\n",
      "All good for page number 66!\n",
      "All good for page number 67!\n",
      "All good for page number 68!\n",
      "All good for page number 69!\n",
      "All good for page number 70!\n",
      "Scraped Successfully! :)\n"
     ]
    }
   ],
   "source": [
    "Scrape(61,71,'scraped_pg61_pg70.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good for page number 71!\n",
      "All good for page number 72!\n",
      "All good for page number 73!\n",
      "All good for page number 74!\n",
      "All good for page number 75!\n",
      "All good for page number 76!\n",
      "All good for page number 77!\n",
      "All good for page number 78!\n",
      "All good for page number 79!\n",
      "All good for page number 80!\n",
      "Scraped Successfully! :)\n"
     ]
    }
   ],
   "source": [
    "Scrape(71,81,'scraped_pg71_pg80.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good for page number 81!\n",
      "All good for page number 82!\n",
      "All good for page number 83!\n",
      "All good for page number 84!\n",
      "All good for page number 85!\n",
      "All good for page number 86!\n",
      "All good for page number 87!\n",
      "All good for page number 88!\n",
      "All good for page number 89!\n",
      "All good for page number 90!\n",
      "Scraped Successfully! :)\n"
     ]
    }
   ],
   "source": [
    "Scrape(81,91,'scraped_pg81_pg90.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good for page number 91!\n",
      "All good for page number 92!\n",
      "All good for page number 93!\n",
      "All good for page number 94!\n",
      "All good for page number 95!\n",
      "All good for page number 96!\n",
      "All good for page number 97!\n",
      "All good for page number 98!\n",
      "All good for page number 99!\n",
      "All good for page number 100!\n",
      "Scraped Successfully! :)\n"
     ]
    }
   ],
   "source": [
    "Scrape(91,101,'scraped_pg91_pg100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good for page number 101!\n",
      "All good for page number 102!\n",
      "All good for page number 103!\n",
      "All good for page number 104!\n",
      "All good for page number 105!\n",
      "All good for page number 106!\n",
      "All good for page number 107!\n",
      "All good for page number 108!\n",
      "All good for page number 109!\n",
      "All good for page number 110!\n",
      "Scraped Successfully! :)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    Scrape(101,111,'scraped_pg101_pg111.csv')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good for page number 111!\n",
      "All good for page number 112!\n",
      "All good for page number 113!\n",
      "Please maunually get info for page114 and link with index 9\n",
      "All good for page number 114!\n",
      "All good for page number 115!\n",
      "All good for page number 116!\n",
      "All good for page number 117!\n",
      "All good for page number 118!\n",
      "All good for page number 119!\n",
      "All good for page number 120!\n",
      "Scraped Successfully! :)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    Scrape(111,121,'scraped_pg111_pg121.csv')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good for page number 121!\n",
      "All good for page number 122!\n",
      "All good for page number 123!\n",
      "All good for page number 124!\n",
      "All good for page number 125!\n",
      "All good for page number 126!\n",
      "All good for page number 127!\n",
      "All good for page number 128!\n",
      "All good for page number 129!\n",
      "All good for page number 130!\n",
      "Scraped Successfully! :)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    Scrape(121,131,'scraped_pg121_pg131.csv')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good for page number 131!\n",
      "All good for page number 132!\n",
      "All good for page number 133!\n",
      "Please maunually get info for page134 and link with index 11\n",
      "All good for page number 134!\n",
      "All good for page number 135!\n",
      "All good for page number 136!\n",
      "All good for page number 137!\n",
      "All good for page number 138!\n",
      "All good for page number 139!\n",
      "All good for page number 140!\n",
      "Scraped Successfully! :)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    Scrape(131,141,'scraped_pg131_pg141.csv')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good for page number 141!\n",
      "All good for page number 142!\n",
      "All good for page number 143!\n",
      "All good for page number 144!\n",
      "All good for page number 145!\n",
      "All good for page number 146!\n",
      "All good for page number 147!\n",
      "All good for page number 148!\n",
      "All good for page number 149!\n",
      "All good for page number 150!\n",
      "Scraped Successfully! :)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    Scrape(141,151,'scraped_pg141_pg151.csv')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good for page number 151!\n",
      "All good for page number 152!\n",
      "All good for page number 153!\n",
      "All good for page number 154!\n",
      "All good for page number 155!\n",
      "All good for page number 156!\n",
      "All good for page number 157!\n",
      "All good for page number 158!\n",
      "All good for page number 159!\n",
      "All good for page number 160!\n",
      "Scraped Successfully! :)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    Scrape(151,161,'scraped_pg151_pg161.csv')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good for page number 161!\n",
      "All good for page number 162!\n",
      "All good for page number 163!\n",
      "All good for page number 164!\n",
      "All good for page number 165!\n",
      "All good for page number 166!\n",
      "All good for page number 167!\n",
      "All good for page number 168!\n",
      "All good for page number 169!\n",
      "All good for page number 170!\n",
      "Scraped Successfully! :)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    Scrape(161,171,'scraped_pg161_pg171.csv')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good for page number 171!\n",
      "All good for page number 172!\n",
      "All good for page number 173!\n",
      "All good for page number 174!\n",
      "All good for page number 175!\n",
      "All good for page number 176!\n",
      "All good for page number 177!\n",
      "All good for page number 178!\n",
      "All good for page number 179!\n",
      "All good for page number 180!\n",
      "All good for page number 181!\n",
      "Please maunually get info for page182 and link with index 30\n",
      "All good for page number 182!\n",
      "All good for page number 183!\n",
      "All good for page number 184!\n",
      "All good for page number 185!\n",
      "All good for page number 186!\n",
      "All good for page number 187!\n",
      "All good for page number 188!\n",
      "All good for page number 189!\n",
      "All good for page number 190!\n",
      "All good for page number 191!\n",
      "All good for page number 192!\n",
      "All good for page number 193!\n",
      "All good for page number 194!\n",
      "All good for page number 195!\n",
      "All good for page number 196!\n",
      "All good for page number 197!\n",
      "All good for page number 198!\n",
      "All good for page number 199!\n",
      "All good for page number 200!\n",
      "Scraped Successfully! :)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    Scrape(171,201,'scraped_pg171_pg201.csv')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good for page number 201!\n",
      "All good for page number 202!\n",
      "All good for page number 203!\n",
      "All good for page number 204!\n",
      "All good for page number 205!\n",
      "All good for page number 206!\n",
      "All good for page number 207!\n",
      "All good for page number 208!\n",
      "All good for page number 209!\n",
      "All good for page number 210!\n",
      "All good for page number 211!\n",
      "All good for page number 212!\n",
      "All good for page number 213!\n",
      "All good for page number 214!\n",
      "All good for page number 215!\n",
      "All good for page number 216!\n",
      "All good for page number 217!\n",
      "All good for page number 218!\n",
      "All good for page number 219!\n",
      "All good for page number 220!\n",
      "All good for page number 221!\n",
      "All good for page number 222!\n",
      "All good for page number 223!\n",
      "Please maunually get info for page224 and link with index 16\n",
      "Please maunually get info for page224 and link with index 17\n",
      "Please maunually get info for page224 and link with index 18\n",
      "All good for page number 224!\n",
      "All good for page number 225!\n",
      "All good for page number 226!\n",
      "All good for page number 227!\n",
      "All good for page number 228!\n",
      "Scraped Successfully! :)\n"
     ]
    }
   ],
   "source": [
    "Scrape(201,229,'scraped_pg201_pg228.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
